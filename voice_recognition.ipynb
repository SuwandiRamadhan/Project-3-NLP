{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11089816",
   "metadata": {},
   "source": [
    "Nama : Suwandi Ramadhan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eefcf0",
   "metadata": {},
   "source": [
    "# Project 3 - Voice Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f4ed7",
   "metadata": {},
   "source": [
    "## Data & Algoritma Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c562cb9",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "| Nama Dataaset | MINDS-14 (Multilingual Intent Navigation and Discovery in 14 languages) |\n",
    "|---------------|-------------------------------------------------------------------------|\n",
    "| Pembuat | PolyAI |\n",
    "| Deskripsi Singkat | Ini adalah dataset audio yang sangat populer untuk melatih dan mengevaluasi model Spoken Language Understanding (SLU), terutama untuk tugas klasifikasi niat (intent classification). Dataset ini berisi rekaman suara orang-orang yang memberikan perintah atau pertanyaan terkait domain perbankan online. |\n",
    "| Isi Dataset | - File audio dalam format .wav <br> - Transkripsi teks dari setiap file audio. <br> - Label niat (intent) untuk setiap rekaman. Contoh niatnya seperti pay_bill (bayar tagihan), transfer (transfer uang), balance (cek saldo), dll. |\n",
    "| Fitur Utama | Multilingual |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a76a14",
   "metadata": {},
   "source": [
    "### Daftar Bahasa dan Kode Konfigurasi\n",
    "\n",
    "Kode bahasa (`name`) :\n",
    "\n",
    "- `cs-CZ` (Czech)\n",
    "- `de-DE` (German)\n",
    "- `en-AU` (English, Australia)\n",
    "- `en-GB` (English, UK)\n",
    "- `en-US` (English, US)\n",
    "- `es-ES` (Spanish)\n",
    "- `fr-FR` (French)\n",
    "- `it-IT` (Italian)\n",
    "- `ko-KR` (Korean)\n",
    "- `nl-NL` (Dutch)\n",
    "- `pl-PL` (Polish)\n",
    "- `pt-PT` (Portuguese)\n",
    "- `ru-RU` (Russian)\n",
    "- `zh-CN` (Chinese, Mandarin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7b167",
   "metadata": {},
   "source": [
    "### Data Fields\n",
    "\n",
    "| Nama | Tipe | Deskripsi |\n",
    "|------|------|-----------|\n",
    "| `path` | string | Path to the audio file |\n",
    "| `audio` | dict | Audio object including loaded audio array, sampling rate and path ot audio |\n",
    "| `transcription` | string | Transcription of the audio file |\n",
    "| `english_transcription` | string | English transcription of the audio file |\n",
    "| `intent_class` | integer | Class id of intent |\n",
    "| `lang_id` | integer | Id of language | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2afd6",
   "metadata": {},
   "source": [
    "### Citation Information\n",
    "\n",
    "`author`     : Daniela Gerz and Pei{-}Hao Su and Razvan Kusztos and Avishek Mondal and Michal Lis and Eshan Singhal and Nikola Mrksic and Tsung{-}Hsien Wen and Ivan Vulic<br> \n",
    "`title`      : Multilingual and Cross-Lingual Intent Detection from Spoken Data<br>\n",
    "`journal`    : CoRR<br>\n",
    "`volume`     : abs/2104.08524<br>\n",
    "`year`       : 2021<br>\n",
    "`url`        : https://arxiv.org/abs/2104.08524<br>\n",
    "`eprinttype` : arXiv<br>\n",
    "`eprint`     : 2104.08524<br>\n",
    "`timestamp`  : Mon, 26 Apr 2021 17:25:10 +0200<br>\n",
    "`biburl`     : https://dblp.org/rec/journals/corr/abs-2104-08524.bib<br>\n",
    "`bibsource`  : dblp computer science bibliography, https://dblp.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4075af",
   "metadata": {},
   "source": [
    "## Algoritma Understanding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493589b2",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d1ac4",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81fcaba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\SUWAAAN\\AppData\\Local\\Temp\\ipykernel_17960\\3468573203.py\", line 9, in <module>\n",
      "    import evaluate\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\evaluate\\__init__.py\", line 29, in <module>\n",
      "    from .evaluation_suite import EvaluationSuite\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\evaluate\\evaluation_suite\\__init__.py\", line 10, in <module>\n",
      "    from ..evaluator import evaluator\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\evaluate\\evaluator\\__init__.py\", line 17, in <module>\n",
      "    from transformers.pipelines import SUPPORTED_TASKS as SUPPORTED_PIPELINE_TASKS\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py\", line 47, in <module>\n",
      "    from .audio_classification import AudioClassificationPipeline\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\transformers\\pipelines\\audio_classification.py\", line 21, in <module>\n",
      "    from .base import Pipeline, build_pipeline_init_args\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 34, in <module>\n",
      "    from ..modelcard import ModelCard\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\transformers\\modelcard.py\", line 48, in <module>\n",
      "    from .training_args import ParallelMode\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\transformers\\training_args.py\", line 76, in <module>\n",
      "    from .trainer_pt_utils import AcceleratorConfig\n",
      "  File \"c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\transformers\\trainer_pt_utils.py\", line 235, in <module>\n",
      "    device: Optional[torch.device] = torch.device(\"cuda\"),\n",
      "c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\transformers\\trainer_pt_utils.py:235: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: Optional[torch.device] = torch.device(\"cuda\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versi Transformers: 4.41.2\n",
      "Versi PyTorch: 2.3.1+cpu\n",
      "Semua library berhasil di-import.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Library dari Hugging Face\n",
    "from datasets import load_dataset, Audio\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForAudioClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "print(\"Versi Transformers:\", transformers.__version__)\n",
    "print(\"Versi PyTorch:\", torch.__version__)\n",
    "print(\"Semua library berhasil di-import.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b76b7a",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b1c28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Memuat Dataset MINDS-14 ---\n",
      "\n",
      "Dataset berhasil dimuat:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n",
      "        num_rows: 563\n",
      "    })\n",
      "})\n",
      "\n",
      "Daftar Niat (Intent): ['abroad', 'address', 'app_error', 'atm_limit', 'balance', 'business_loan', 'card_issues', 'cash_deposit', 'direct_debit', 'freeze', 'high_value_payment', 'joint_account', 'latest_transactions', 'pay_bill']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Memuat Dataset MINDS-14 ---\")\n",
    "\n",
    "# Muat dataset untuk bahasa Inggris (AS)\n",
    "dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", trust_remote_code=True)\n",
    "\n",
    "# Pastikan semua audio memiliki sampling rate 16kHz, standar untuk model Wav2Vec2\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "print(\"\\nDataset berhasil dimuat:\")\n",
    "print(dataset)\n",
    "\n",
    "# Membuat pemetaan dari ID ke label (dan sebaliknya) untuk memudahkan interpretasi\n",
    "labels = dataset[\"train\"].features[\"intent_class\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "print(\"\\nDaftar Niat (Intent):\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e09ef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Memuat Model Pre-trained Wav2Vec2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model dan Feature Extractor berhasil dimuat.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Memuat Model Pre-trained Wav2Vec2 ---\")\n",
    "model_checkpoint = \"facebook/wav2vec2-base\"\n",
    "\n",
    "# Feature extractor akan memproses sinyal audio mentah menjadi format yang dimengerti model\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Muat model dengan kepala klasifikasi di atasnya.\n",
    "# Kepala klasifikasi ini masih acak dan perlu dilatih (fine-tune).\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "print(\"\\nModel dan Feature Extractor berhasil dimuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a115c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Melakukan Pre-processing Data ---\n",
      "\n",
      "Pre-processing selesai.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Melakukan Pre-processing Data ---\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Fungsi untuk mengubah data audio mentah menjadi 'input_values' yang siap\n",
    "    digunakan oleh model.\n",
    "    \"\"\"\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=16000 * 5,  # Batasi durasi audio menjadi 5 detik\n",
    "        truncation=True,\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "# Terapkan fungsi preprocessing ke seluruh dataset\n",
    "# Kita juga menghapus kolom yang tidak diperlukan untuk training\n",
    "encoded_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=[\"audio\", \"transcription\", \"english_transcription\", \"lang_id\"],\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Ganti nama kolom 'intent_class' menjadi 'label' karena Trainer API mencarinya\n",
    "encoded_dataset = encoded_dataset.rename_column(\"intent_class\", \"label\")\n",
    "print(\"\\nPre-processing selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac1bb86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Memisahkan data menjadi Train dan Eval ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to avoid copy while creating an array as requested.\nIf using `np.array(obj, copy=False)` replace it with `np.asarray(obj)` to allow a copy when needed (no behavior change in NumPy 1.x).\nFor more details, see https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Memisahkan data menjadi Train dan Eval ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Ini adalah praktik terbaik untuk mendapatkan evaluasi performa model yang jujur\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Kita gunakan 20% data untuk validasi\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m splits = \u001b[43mencoded_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify_by_column\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m train_dataset = splits[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m eval_dataset = splits[\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:567\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    560\u001b[39m self_format = {\n\u001b[32m    561\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    562\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    563\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    564\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    565\u001b[39m }\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m datasets: List[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    569\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\datasets\\fingerprint.py:482\u001b[39m, in \u001b[36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:4658\u001b[39m, in \u001b[36mDataset.train_test_split\u001b[39m\u001b[34m(self, test_size, train_size, shuffle, stratify_by_column, seed, generator, keep_in_memory, load_from_cache_file, train_indices_cache_file_name, test_indices_cache_file_name, writer_batch_size, train_new_fingerprint, test_new_fingerprint)\u001b[39m\n\u001b[32m   4651\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4652\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe least populated class in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstratify_by_column\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m column has only 1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4653\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m member, which is too few. The minimum\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4654\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m number of groups for any class cannot\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4655\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be less than 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4656\u001b[39m             )\n\u001b[32m   4657\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4658\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m   4660\u001b[39m \u001b[38;5;66;03m# random partition\u001b[39;00m\n\u001b[32m   4661\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4662\u001b[39m     permutation = generator.permutation(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:4646\u001b[39m, in \u001b[36mDataset.train_test_split\u001b[39m\u001b[34m(self, test_size, train_size, shuffle, stratify_by_column, seed, generator, keep_in_memory, load_from_cache_file, train_indices_cache_file_name, test_indices_cache_file_name, writer_batch_size, train_new_fingerprint, test_new_fingerprint)\u001b[39m\n\u001b[32m   4640\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4641\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStratifying by column is only supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mClassLabel.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m column, and column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstratify_by_column\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m._info.features[stratify_by_column]).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4642\u001b[39m     )\n\u001b[32m   4643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   4644\u001b[39m     train_indices, test_indices = \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   4645\u001b[39m         stratified_shuffle_split_generate_indices(\n\u001b[32m-> \u001b[39m\u001b[32m4646\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwith_format\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnumpy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstratify_by_column\u001b[49m\u001b[43m]\u001b[49m, n_train, n_test, rng=generator\n\u001b[32m   4647\u001b[39m         )\n\u001b[32m   4648\u001b[39m     )\n\u001b[32m   4649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m   4650\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) == \u001b[33m\"\u001b[39m\u001b[33mMinimum class count error\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:2861\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2859\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   2860\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2861\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:2846\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2844\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m   2845\u001b[39m pa_subtable = query_table(\u001b[38;5;28mself\u001b[39m._data, key, indices=\u001b[38;5;28mself\u001b[39m._indices)\n\u001b[32m-> \u001b[39m\u001b[32m2846\u001b[39m formatted_output = \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[32m   2848\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2849\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py:633\u001b[39m, in \u001b[36mformat_table\u001b[39m\u001b[34m(table, key, formatter, format_columns, output_all_columns)\u001b[39m\n\u001b[32m    631\u001b[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    635\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py:399\u001b[39m, in \u001b[36mFormatter.__call__\u001b[39m\u001b[34m(self, pa_table, query_type)\u001b[39m\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_row(pa_table)\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_batch(pa_table)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\datasets\\formatting\\np_formatter.py:94\u001b[39m, in \u001b[36mNumpyFormatter.format_column\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa.Table) -> np.ndarray:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     column = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     column = \u001b[38;5;28mself\u001b[39m.python_features_decoder.decode_column(column, pa_table.column_names[\u001b[32m0\u001b[39m])\n\u001b[32m     96\u001b[39m     column = \u001b[38;5;28mself\u001b[39m.recursive_tensorize(column)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py:162\u001b[39m, in \u001b[36mNumpyArrowExtractor.extract_column\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa.Table) -> np.ndarray:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_arrow_array_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\0. Bootcamp AI\\Repository\\Bootcamp-AI\\Project 3\\Project 3\\.venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py:197\u001b[39m, in \u001b[36mNumpyArrowExtractor._arrow_array_to_numpy\u001b[39m\u001b[34m(self, pa_array)\u001b[39m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    192\u001b[39m         (\u001b[38;5;28misinstance\u001b[39m(x, np.ndarray) \u001b[38;5;129;01mand\u001b[39;00m (x.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m x.shape != array[\u001b[32m0\u001b[39m].shape))\n\u001b[32m    193\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m np.isnan(x))\n\u001b[32m    194\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m array\n\u001b[32m    195\u001b[39m     ):\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.array(array, copy=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=\u001b[38;5;28mobject\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: Unable to avoid copy while creating an array as requested.\nIf using `np.array(obj, copy=False)` replace it with `np.asarray(obj)` to allow a copy when needed (no behavior change in NumPy 1.x).\nFor more details, see https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword."
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Memisahkan data menjadi Train dan Eval ---\")\n",
    "# Ini adalah praktik terbaik untuk mendapatkan evaluasi performa model yang jujur\n",
    "# Kita gunakan 20% data untuk validasi\n",
    "splits = encoded_dataset[\"train\"].train_test_split(test_size=0.2, stratify_by_column=\"label\")\n",
    "\n",
    "train_dataset = splits['train']\n",
    "eval_dataset = splits['test']\n",
    "\n",
    "print(\"\\nJumlah data Latihan:\", len(train_dataset))\n",
    "print(\"Jumlah data Validasi:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f29ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Mengkonfigurasi Proses Training ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_metric.compute(predictions=predictions, references=eval_pred.label_ids)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Tentukan argumen-argumen untuk training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./voice_recognition_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Evaluasi model di setiap akhir epoch\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Simpan model di setiap akhir epoch\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Jumlah epoch (bisa dimulai dengan 3 untuk percobaan cepat)\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Di akhir, model terbaik akan dimuat secara otomatis\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Mengkonfigurasi Proses Training ---\")\n",
    "# Definisikan metrik untuk evaluasi\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Menghitung akurasi selama evaluasi.\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "# Tentukan argumen-argumen untuk training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./voice_recognition_model\",\n",
    "    evaluation_strategy=\"epoch\",      # Evaluasi model di setiap akhir epoch\n",
    "    save_strategy=\"epoch\",            # Simpan model di setiap akhir epoch\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,               # Jumlah epoch (bisa dimulai dengan 3 untuk percobaan cepat)\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,      # Di akhir, model terbaik akan dimuat secara otomatis\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc94b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=feature_extractor, # Feature extractor diteruskan sebagai tokenizer\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Memulai Proses Training ---\")\n",
    "# Mulai proses fine-tuning!\n",
    "trainer.train()\n",
    "print(\"\\n--- Proses Training Selesai ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f68a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Menyimpan dan Menguji Model Final ---\")\n",
    "\n",
    "# Simpan model terbaik yang sudah dilatih\n",
    "NAMA_MODEL_FINAL = \"model_voice_recognition_final\"\n",
    "trainer.save_model(NAMA_MODEL_FINAL)\n",
    "print(f\"Model final disimpan di folder: {NAMA_MODEL_FINAL}\")\n",
    "\n",
    "# Gunakan 'pipeline' untuk cara termudah melakukan prediksi\n",
    "pipe = pipeline(\"audio-classification\", model=NAMA_MODEL_FINAL)\n",
    "\n",
    "# Pilih sampel acak dari data validasi (yang tidak pernah dilihat saat training)\n",
    "random_sample = random.choice(eval_dataset)\n",
    "file_audio_untuk_tes = random_sample[\"path\"]\n",
    "label_asli_id = random_sample[\"label\"]\n",
    "label_asli_nama = id2label[str(label_asli_id)]\n",
    "\n",
    "print(f\"\\nMelakukan prediksi pada file audio: {file_audio_untuk_tes}\")\n",
    "\n",
    "# Lakukan prediksi\n",
    "hasil_prediksi = pipe(file_audio_untuk_tes)\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(\"\\n--- HASIL PREDIKSI ---\")\n",
    "print(f\"Label Asli        : {label_asli_nama}\")\n",
    "# Prediksi teratas adalah yang pertama di dalam list hasil\n",
    "prediksi_teratas = hasil_prediksi[0]\n",
    "print(f\"Prediksi Model    : {prediksi_teratas['label']} (dengan skor: {prediksi_teratas['score']:.4f})\")\n",
    "\n",
    "print(\"\\nSemua hasil prediksi:\")\n",
    "print(hasil_prediksi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
